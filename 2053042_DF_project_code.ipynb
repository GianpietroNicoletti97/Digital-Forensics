{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3s4WIyyoMQ9"
      },
      "source": [
        "#Digital Forensics Final Project\n",
        "\n",
        "\n",
        "Project 4: Anomaly Detection and Classification\n",
        "\n",
        "---\n",
        "**University of Padua**<br>\n",
        "*Master Degree in ICT for Internet and Multimedia - Cybersystems*\n",
        "<br>\n",
        "<br>\n",
        "**Student:** *Gianpietro Nicoletti*\n",
        "<br>\n",
        "**ID number:** 2053042\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTES FOR THE READER:** <br>\n",
        " \n",
        "\n",
        "*   After the initial configuration, the reader can simply run all cell and wait for the results;\n",
        "*   The datasets and the models will be dowloaded, eventually, from a google drive folder;\n",
        "*   Autoencoder results are intended as the recostructed images while for the SVM and the Isolation Forest they are intended as the output labels;\n",
        "*   For the Isolation Forest optimized and for the Classificator network the results were not saved since their computation is very fast;\n",
        "*   For the same reason the optimezed Isolation Forest and its features space were not saved.\n",
        "*   Use the runtime with GPU and clean the memory before lauching the code since, the program (in some parts), may stop and wait for user input if it needs generate some folders and they are already present in the memory;\n",
        "*   A shor report about this project can be found [HERE](https://drive.google.com/file/d/161VtEO7X8ret0o0tKnuO3Yq4uosdSgyX/view?usp=sharing).\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGGwaQwvBZj1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdtk_RytMNht"
      },
      "source": [
        "##Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPjxE-62eZ5b"
      },
      "source": [
        "###Program settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-bLjLaYd22q"
      },
      "outputs": [],
      "source": [
        "                           ####################\n",
        "############################ RUNNING SETTINGS ############################\n",
        "                           ####################\n",
        "\n",
        "#Eneble the debug messages:\n",
        "DEBUG_MODE = True \n",
        "\n",
        "#Use google drive as local memory\n",
        "GOOGLE_DRIVE = False\n",
        "\n",
        "                           ####################                                  \n",
        "############################ DATASET SETTINGS ############################\n",
        "                           #################### \n",
        "\n",
        "#Use the dataset already created before => faster exectution of the code: \n",
        "#set it to true if you want use the default dataset \n",
        "#(with 7000 samples for train and 3000 samples for test):                              \n",
        "DATASETS_ALREADY_CREATED = True\n",
        "                                  \n",
        "#Download the datasets just computed in the local memory:\n",
        "DOWNLOAD_DATASET = False\n",
        "\n",
        "#Number of images to be used for the training phase:\n",
        "IMAGES_TRAIN = 7000\n",
        "\n",
        "#Number of images to be used for the test phase:\n",
        "IMAGES_TEST = 3000\n",
        "\n",
        "\n",
        "                           ##################\n",
        "############################ MODEL SETTINGS ##############################\n",
        "                           ##################\n",
        "\n",
        "####################\n",
        "# ISOLATION FOREST #\n",
        "####################\n",
        "\n",
        "#Select the number of estimators in the isolation forest model\n",
        "N_ESTIMATORS = 50\n",
        "\n",
        "#Load the already fitted isolation forest model:\n",
        "LOAD_IF = True\n",
        "\n",
        "#Load the already computed isolation forest model result:\n",
        "LOAD_IF_RESULTS =  True\n",
        "\n",
        "#Download the results obtained by the model in the local memory:\n",
        "DOWNLOAD_RESULTS_IF = False\n",
        "\n",
        "#Download the fitted model in the local memory:\n",
        "DOWNLOAD_IF_MODEL = False\n",
        "\n",
        "#######\n",
        "# SVM #\n",
        "#######\n",
        "\n",
        "#Load the already fitted one-class SVM model (with SGD):\n",
        "LOAD_SVM =  True\n",
        "\n",
        "#Use stochastics gradient descent to train the SVM model \n",
        "SGD_SVM = True\n",
        "\n",
        "#Load the already computed one-class SVM model result:\n",
        "LOAD_SVM_RESULTS = True\n",
        "\n",
        "#Download the results obtained by the model:\n",
        "DOWNLOAD_RESULTS_SVM = False\n",
        "\n",
        "#Download the fitted model in the local memory:\n",
        "DOWNLOAD_SVM_MODEL = False\n",
        "\n",
        "###############\n",
        "# Autoencoder #\n",
        "###############\n",
        "\n",
        "#Select the number of epochs perfomed during the train phase:\n",
        "N_EPOCHS = 100\n",
        "\n",
        "#Select the batch size during the train phase:\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "#Load the already fitted autoencoder:\n",
        "LOAD_AUTOENCODER = True\n",
        "\n",
        "#Load the already computed autoencoder result:\n",
        "LOAD_AUTOENCODER_RESULTS = True\n",
        "\n",
        "#Download the results obtained by the model:\n",
        "DOWNLOAD_RESULTS_AUTOENCODER = False\n",
        "\n",
        "#Download the fitted model in the local memory:\n",
        "DOWNLOAD_AUTOENCODER_MODEL = False\n",
        "\n",
        "##################\n",
        "# CLASSIFICATION #\n",
        "##################\n",
        "\n",
        "#Select the number of epochs perfomed during the train phase:\n",
        "N_EPOCHS_C = 100\n",
        "\n",
        "#Select the batch size during the train phase:\n",
        "BATCH_SIZE_C = 32\n",
        "\n",
        "#Load the already fitted classificator:\n",
        "LOAD_CLASSIFICATOR = True\n",
        "\n",
        "#Download the fitted model in the local memory:\n",
        "DOWNLOAD_CLASSIFICATOR_MODEL = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aID9Qq2nB8en"
      },
      "outputs": [],
      "source": [
        "if(GOOGLE_DRIVE):\n",
        "  \n",
        "  drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnRsKIQsoikx"
      },
      "source": [
        "###Import of the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfbferZkNUsX"
      },
      "outputs": [],
      "source": [
        "                           ###################\n",
        "############################ CLASSIC CLASSES ############################\n",
        "                           ###################\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sn\n",
        "from scipy.spatial import distance\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "                             ##############\n",
        "############################## TENSORFLOW ################################\n",
        "                             ##############\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D,Conv2DTranspose, Flatten, Reshape, LeakyReLU, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras import losses\n",
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "\n",
        "                          #####################\n",
        "########################### IMAGES AND MEMORY ############################\n",
        "                          #####################\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import gc\n",
        "\n",
        "                              ###########\n",
        "############################### SKLEARN ##################################\n",
        "                              ###########\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.linear_model import SGDOneClassSVM\n",
        "\n",
        "                               ###########\n",
        "################################ VARIOUS #################################\n",
        "                               ###########                 \n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition of output class"
      ],
      "metadata": {
        "id": "p1ZsLix-BDlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFdJM99SOTv9"
      },
      "outputs": [],
      "source": [
        "#utility class to improve the readability of the output texts\n",
        "class output:\n",
        "    HEADER = \"\\033[95m\"\n",
        "    OKBLUE = \"\\033[94m\"\n",
        "    OKGREEN = \"\\033[92m\"\n",
        "    WARNING = \"\\033[93m\"\n",
        "    FAIL = \"\\033[91m\"\n",
        "    END = \"\\033[0m\"\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNDERLINE = \"\\033[4m\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnoZFTobr8KM"
      },
      "source": [
        " ### Downloading of the pre-computed datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXeq67Nrr1VQ"
      },
      "outputs": [],
      "source": [
        "#use the precomputed datasets\n",
        "if(DATASETS_ALREADY_CREATED):\n",
        "\n",
        "  #fixing the number of samples\n",
        "  IMAGES_TRAIN = 7000\n",
        "  IMAGES_TEST = 3000\n",
        "\n",
        "  !gdown --id 18LgmmOYoJsiaf2-fiKFa0qtvNpUrxVOZ #X_train.npy\n",
        "  X_train_vector = np.load(\"X_train.npy\", mmap_mode = \"r\")\n",
        "\n",
        "  !gdown --id 1WVgrwRu_A5KL5H2fAqDGxacbFcGbR7l5 #X_train_classification.npy\n",
        "  !gdown --id 100qBozj2WQMbr90MSSBYliHmJneTgE4R #Y_train_classification.npy\n",
        "  X_train_classification = np.load(\"X_train_classification.npy\", mmap_mode = \"r\")\n",
        "  Y_train_classification = np.load(\"Y_train_classification.npy\", mmap_mode = \"r\")\n",
        "\n",
        "\n",
        "  !gdown --id 1HKkuNT_HZnHbqmWpSsuNlHM8qRleITIm #X_test.npy\n",
        "  !gdown --id 1BEOIBa50i9DDKPMSQWJMSfzlqZ3AU4-2 #Y_test.npy\n",
        "  X_test_vector = np.load(\"X_test.npy\", mmap_mode = \"r\")\n",
        "  Y_test=np.load(\"Y_test.npy\",mmap_mode = \"r\")\n",
        "\n",
        "  !gdown --id 15rOBB22BfuYzal6wDg8EJQbXXs2FgXX5 #X_train_matrix.npy\n",
        "  !gdown --id 1lbTIbg-Z2A04VfxujVhZfToDIm8Fa07r #X_test_matrix.npy\n",
        "  X_train_matrix = np.load(\"X_train_matrix.npy\", mmap_mode = \"r\")\n",
        "  X_test_matrix=  np.load(\"X_test_matrix.npy\",mmap_mode = \"r\")\n",
        "\n",
        "  print(output.OKGREEN+str(len(X_train_vector))+\" images loaded for the train set\"+output.END)\n",
        "  print(output.OKGREEN+str(len(X_test_vector))+\" images loaded for the test set\"+output.END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EOPx1UNeBmC"
      },
      "source": [
        "###Setting up the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaJoqJGQjnIM"
      },
      "outputs": [],
      "source": [
        "#### DOWNLOAD OF THE TRAIN SET ####\n",
        "\n",
        "if(not DATASETS_ALREADY_CREATED): #download the full dataset\n",
        "\n",
        "  !gdown --id 1xfh_edTic07EGyFeI98O8FoCKkdQ5de4 #train.npz\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnix-dnVPp9W"
      },
      "outputs": [],
      "source": [
        "if(not DATASETS_ALREADY_CREATED):\n",
        "  #unzip the train dataset and delete the compressed folder\n",
        "  \n",
        "  !unzip train.npz\n",
        "  !rm train.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHxbgHUgNeHv"
      },
      "outputs": [],
      "source": [
        "if(not DATASETS_ALREADY_CREATED):\n",
        "\n",
        "  #read the dataset: each picture is seen as a vector of lenght 65536 \n",
        "  X_train_temp = np.load(\"arr_0.npy\", mmap_mode = \"r\")\n",
        "\n",
        "  print(output.OKGREEN+str(X_train_temp.shape[0])+\" images loaded for the train set \"+output.END)\n",
        "\n",
        "  IMAGES_TRAIN = min(IMAGES_TRAIN, X_train_temp.shape[0])  # i can not add more images than the images avaiable\n",
        "\n",
        "  print(output.WARNING+\"Only \"+str(IMAGES_TRAIN)+\" images will be used\"+output.END)\n",
        "\n",
        "\n",
        "\n",
        "############## TRAIN SET FOR ANOMALY DETECTION #################\n",
        "\n",
        "  #generation of the accepted indexes and sorting them\n",
        "  indexes = random.sample(range(X_train_temp.shape[0]), IMAGES_TRAIN)\n",
        "  indexes.sort()\n",
        "\n",
        "  X_train_vector = X_train_temp[indexes]\n",
        "\n",
        "\n",
        "  #convert the array into a memory-map array\n",
        "  np.save(\"X_train\", X_train_vector)\n",
        "\n",
        "  #cleaning ram\n",
        "  del X_train_vector\n",
        "  gc.collect()\n",
        "\n",
        "  X_train_vector = np.load(\"X_train.npy\", mmap_mode = \"r+\")\n",
        "\n",
        "\n",
        "\n",
        "#### DEBUG ####\n",
        "\n",
        "if(DEBUG_MODE):\n",
        "\n",
        "  print(output.BOLD+\"\\n\\nCHECK\"+output.END)\n",
        "\n",
        "  if (len(X_train_vector) != IMAGES_TRAIN):\n",
        "\n",
        "    print(output.WARNING+\"\\nERROR: Number of samples: \"+str(len(X_train_vector))+\" but \"+str(IMAGES_TRAIN)+\" were expected\"+output.END)\n",
        "\n",
        "  else:\n",
        "\n",
        "    print(output.OKGREEN+\"\\nNumber of samples: \"+str(len(X_train_vector)))\n",
        "\n",
        "  if(\"(65536,)\" == str(X_train_vector[0].shape)):\n",
        "\n",
        "    print(output.OKGREEN+\"Shape of vector samples: \"+str(X_train_vector[0].shape)+output.END)\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    print(output.WARNING+\"ERROR: Shape of vector samples: \"+str(X_train_vector[0].shape)+\" but (65536,) was expected\"+output.END)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bvSZ0JbQMHw"
      },
      "source": [
        "### Setting up the test set and train set for the classification module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCWrY-XAQI9u"
      },
      "outputs": [],
      "source": [
        "#### DOWNLOAD OF THE TEST SET ####\n",
        "if(not DATASETS_ALREADY_CREATED):\n",
        "  \n",
        "  !gdown --id 1uxwI0AUewcH67c2h17lB-9IOV4Ag5T-d #test.npz\n",
        "  !gdown --id 1cVDr4Uv4jaiyYl4NS1yTiutxrjzCn0NS #test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyD5SQa2RZ7N"
      },
      "outputs": [],
      "source": [
        "if(not DATASETS_ALREADY_CREATED):\n",
        "\n",
        "  #remove the .npy file created for the train set in order to avoid that the program asks what to do with the new arr_0.npy file\n",
        "  !rm arr_0.npy\n",
        "  \n",
        "  #unzip the test dataset and delete the compressed folder\n",
        "  !unzip test.npz\n",
        "  !rm test.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp16ugh2Voxm"
      },
      "outputs": [],
      "source": [
        "if(not DATASETS_ALREADY_CREATED):\n",
        "\n",
        "  #read the dataset: each picture is seen as a vector of length 65536\n",
        "  X_test_temp = np.load(\"arr_0.npy\", mmap_mode = \"r\")\n",
        "\n",
        "  print(output.OKGREEN+str(X_test_temp.shape[0])+\" images loaded for the test set\"+output.END)\n",
        "\n",
        "  IMAGES_TEST = min(IMAGES_TEST, X_test_temp.shape[0])  # i can not add more images than the images avaiable\n",
        "\n",
        "  print(output.WARNING+\"Only \"+str(IMAGES_TEST)+\" images will be used\"+output.END)\n",
        "\n",
        "\n",
        "############## TRAIN SET FOR CLASSIFICATION #################  \n",
        "\n",
        "\n",
        "  #generation of the accepted indexes and sorting them\n",
        "  indexes = random.sample(range(X_test_temp.shape[0]), IMAGES_TRAIN)\n",
        "  indexes.sort()\n",
        "\n",
        "  X_train_classification = X_test_temp[indexes].reshape(IMAGES_TRAIN,256,256,1).astype(\"float32\")\n",
        "\n",
        "  Y_train_classification = pd.read_csv(\"test.csv\")[\"fine_labels\"].to_numpy()[indexes]\n",
        "\n",
        "  #convert the 2 lists into 2 memory-map arrays\n",
        "\n",
        "  np.save(\"X_train_classification\", X_train_classification)\n",
        "\n",
        "  #cleaning ram\n",
        "  del X_train_classification\n",
        "  gc.collect()\n",
        "\n",
        "  X_train_classification = np.load(\"X_train_classification.npy\", mmap_mode = \"r+\")\n",
        "\n",
        "\n",
        "  np.save(\"Y_train_classification\", Y_train_classification)\n",
        "\n",
        "  #cleaning ram\n",
        "  del Y_train_classification\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "  Y_train_classification = np.load(\"Y_train_classification.npy\", mmap_mode = \"r+\")\n",
        "\n",
        "\n",
        "############## TEST SET #################  \n",
        "\n",
        "\n",
        "  #generation of the accepted indexes and sorting them\n",
        "  indexes = random.sample(range(X_test_temp.shape[0]), IMAGES_TEST)\n",
        "  indexes.sort()\n",
        "\n",
        "  X_test_vector = X_test_temp[indexes]#traintest split!\n",
        "\n",
        "  Y_test = pd.read_csv(\"test.csv\")[\"fine_labels\"].to_numpy()[indexes]\n",
        "\n",
        "  #convert the 2 lists into 2 memory-map arrays\n",
        "\n",
        "  np.save(\"X_test\", X_test_vector)\n",
        "\n",
        "  #cleaning ram\n",
        "  del X_test_vector\n",
        "  gc.collect()\n",
        "\n",
        "  X_test_vector = np.load(\"X_test.npy\", mmap_mode = \"r+\")\n",
        "\n",
        "\n",
        "  np.save(\"Y_test.npy\", Y_test)\n",
        "\n",
        "  #cleaning ram\n",
        "  del Y_test\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "  Y_test = np.load(\"Y_test.npy\", mmap_mode = \"r+\")\n",
        "\n",
        "\n",
        "\n",
        "#### DEBUG ####\n",
        "if(DEBUG_MODE):\n",
        "\n",
        "  print(output.BOLD+\"\\n\\nCHECK\"+output.END)\n",
        "\n",
        "  if (len(X_test_vector) != IMAGES_TEST):\n",
        "\n",
        "    print(output.WARNING+\"\\nERROR: Number of samples: \"+str(len(X_test_vector))+\" but \"+str(IMAGES_TEST)+\" were expected\"+output.END)\n",
        "\n",
        "  else:\n",
        "\n",
        "    print(output.OKGREEN+\"\\nNumber of samples: \"+str(len(X_test_vector)))\n",
        "\n",
        "  if(\"(65536,)\" == str(X_test_vector[0].shape)):\n",
        "\n",
        "    print(output.OKGREEN+\"Shape of samples: \"+str(X_test_vector[0].shape)+output.END)\n",
        "\n",
        "  else:\n",
        "\n",
        "    print(output.WARNING+\"ERROR: Shape of samples: \"+str(X_test_vector[0].shape)+\" but (65536,) was expected\"+output.END)\n",
        "\n",
        "\n",
        "\n",
        "  print(\"\\nNumber of images = number of labels?\")\n",
        "\n",
        "  if(len(X_test_vector) == len(Y_test)):\n",
        "\n",
        "    print(output.OKGREEN+\"YES\"+output.END)\n",
        "\n",
        "  else:\n",
        "    \n",
        "    print(output.WARNING+\"NO\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"clean\",\"dos11\",\"dos53\",\"scan11\",\"scan44\"]\n",
        "y = np.unique(Y_test, return_counts=True)[1]\n",
        "\n",
        "fig, ax = plt.subplots()    \n",
        "width = 0.75 # the width of the bars \n",
        "ind = np.arange(len(y))  # the x locations for the groups\n",
        "ax.barh(ind, y, width, color=[\"blue\",\"red\",\"red\",\"red\", \"red\"])\n",
        "ax.set_yticks(ind+width/2)\n",
        "ax.set_yticklabels(x, minor=False)\n",
        "plt.title(\"Test set data distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")      \n",
        "for i, v in enumerate(y):\n",
        "    ax.text(v + 3, i + .25, str(v), color=\"black\", fontweight=\"bold\")\n",
        "plt.savefig(os.path.join(\"test.png\"), dpi=300, format=\"png\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "tAP1qGQOjYHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_K5N-w4rodm"
      },
      "source": [
        "###Cleaning and download\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OltuqvlYkUKj"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_DATASET):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp X_train.npy /content/drive/MyDrive\n",
        "    !cp X_train_classification.npy /content/drive/MyDrive\n",
        "    !cp Y_train_classification.npy /content/drive/MyDrive\n",
        "    !cp X_test.npy /content/drive/MyDrive\n",
        "    !cp Y_test.npy /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "    \n",
        "    files.download(\"X_train.npy\")\n",
        "    files.download(\"X_train_classification.npy\")\n",
        "    files.download(\"Y_train_classification.npy\")\n",
        "    files.download(\"Y_test.npy\")\n",
        "    files.download(\"X_test.npy\")\n",
        "\n",
        "  print(output.OKGREEN+\"Datasets downloaded to local memory\"+output.END)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuUMh5V5amTS"
      },
      "outputs": [],
      "source": [
        "#delete unused folder and file\n",
        "!rm arr_0.npy\n",
        "!rm Y_test.npy\n",
        "!rm X_test.npy\n",
        "!rm X_train.npy\n",
        "!rm X_train_classification.npy\n",
        "!rm Y_train_classification.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icV156GgMJCc"
      },
      "source": [
        "##Isolation Forest module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj0HgPADPALm"
      },
      "source": [
        "###Generation of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC9VcCvPpbNW"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_IF): #if you don't want load a pre-computed model you need to generate it again\n",
        "\n",
        "  print(output.UNDERLINE+str(N_ESTIMATORS)+\" is the number of estimators selected\"+output.END)\n",
        "\n",
        "  IF_detector = IsolationForest(n_estimators = N_ESTIMATORS, \n",
        "                                n_jobs = -1, random_state = 2053042)\n",
        "  \n",
        "  IF_detector.fit(X_train_vector)\n",
        "\n",
        "  with open(\"IF_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(IF_detector, f)\n",
        "  print(output.OKGREEN+\"Isolation forest model saved\"+output.END)\n",
        "\n",
        "else: #load the pre-computed model\n",
        "\n",
        "  !gdown --id 1Rf6vDCYFLkDenPR63dCYmK4y4kJCgIJQ #IF_model.pkl\n",
        "\n",
        "  with open(\"IF_model.pkl\", \"rb\") as f:\n",
        "    IF_detector = pickle.load(f)\n",
        "  print(output.OKGREEN+\"Isolation forest model loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlSfu3Og8f14"
      },
      "source": [
        "###Evaluation of the perfomace: training error and test error "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo5ZfHYo8tZ9"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_IF_RESULTS):\n",
        "\n",
        "  Y_train_predicted = IF_detector.predict(X_train_vector)\n",
        "  \n",
        "  print(output.OKGREEN+\"Isolation forest train results computed\"+output.END)\n",
        "\n",
        "else:\n",
        "\n",
        "  !gdown --id 13ePhp_roXEzKhmhk3kmNO5Me-BVC5obH #IF_results.npz\n",
        "  Y_train_predicted = np.load(\"IF_results.npz\", mmap_mode = \"r\")[\"Y_train_predicted\"]\n",
        "\n",
        "  print(output.OKGREEN+\"Isolation forest train results loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25GxxdNYbqit"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for prediction in Y_train_predicted:\n",
        "\n",
        "  if(prediction == 1):\n",
        "    \n",
        "    correct = correct + 1;\n",
        "\n",
        "print(\"TRAIN ACCURACY: \" +str(correct/IMAGES_TRAIN*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnVtTzdbcFPf"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_IF_RESULTS): #decide if load results or not\n",
        "\n",
        "  Y_test_predicted = IF_detector.predict(X_test_vector)\n",
        "\n",
        "  print(output.OKGREEN+\"Isolation forest test results computed\"+output.END)\n",
        "\n",
        "else:\n",
        "\n",
        "  Y_test_predicted = np.load(\"IF_results.npz\", mmap_mode = \"r\")[\"Y_test_predicted\"]\n",
        "  \n",
        "  print(output.OKGREEN+\"Isolation forest test results loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Q6eBZEtnai"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "j = 0\n",
        "\n",
        "for prediction in Y_test_predicted:\n",
        "\n",
        "  if(Y_test[j] == 0 and prediction == 1):\n",
        "\n",
        "    correct = correct+1\n",
        "\n",
        "  if(prediction == -1 and Y_test[j] != 0):\n",
        "    \n",
        "    correct = correct + 1\n",
        "  \n",
        "  j = j + 1  \n",
        "\n",
        "print(\"TRAIN ACCURACY: \"+str(correct/IMAGES_TEST*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpF1PYHZbB3r"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_IF_RESULTS):\n",
        "  \n",
        "  np.savez(\"IF_results.npz\", Y_train_predicted = Y_train_predicted , Y_test_predicted = Y_test_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjPcdNfmdBz_"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_RESULTS_IF):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "    !cp IF_results.npz /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "    files.download(\"IF_results.npz\")\n",
        "  \n",
        "\n",
        "  print(output.OKGREEN+\"Isolation forest results downloaded to local memory\"+output.END)\n",
        "\n",
        "if(DOWNLOAD_IF_MODEL):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp IF_model.pkl /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "    \n",
        "    files.download(\"IF_model.pkl\")\n",
        "\n",
        "  print(output.OKGREEN+\"Isolation forest model downloaded to local memory\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Vwgr_LZG2y"
      },
      "outputs": [],
      "source": [
        "#cleaning ram\n",
        "del Y_train_predicted\n",
        "del Y_test_predicted\n",
        "del IF_detector\n",
        "gc.collect()\n",
        "\n",
        "#removed not used files\n",
        "!rm IF_results.npz\n",
        "!rm IF_model.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRIywU3oHnSn"
      },
      "source": [
        "##One-class SVM module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I6cGKHdHioo"
      },
      "source": [
        "###Model generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2-fuX1FGnLl"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_SVM): #if you don't want load a pre-computed model you need to generate it again\n",
        "\n",
        "  if(SGD_SVM):\n",
        "\n",
        "    SVM_detector = SGDOneClassSVM(max_iter = 100000, shuffle = False, random_state = 2053042)\n",
        "    print(output.OKBLUE+\"SGD will be used to train the SVM model\"+output.END)\n",
        "\n",
        "  else:\n",
        "\n",
        "    SVM_detector = OneClassSVM(kernel = \"rbf\", max_iter = 100000)\n",
        "    print(output.OKBLUE+\"SVM model will be train without SGD\"+output.END)\n",
        "\n",
        "  SVM_detector.fit(X_train_vector)\n",
        "\n",
        "  with open(\"SVM_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(SVM_detector, f)\n",
        "  print(output.OKGREEN+\"SVM model saved\"+output.END)\n",
        "\n",
        "else: #load the pre-computed model\n",
        "\n",
        "  !gdown --id 1LuMxx3OBjld_UpE6xTCwdHdW03f7JyDJ #SVM_model.pkl\n",
        "  \n",
        "  with open(\"SVM_model.pkl\", \"rb\") as f:\n",
        "    SVM_detector = pickle.load(f)\n",
        "  print(output.OKGREEN+\"SVM model loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edgsUG49H3El"
      },
      "source": [
        "###Evaluation of the perfomace: training error and test error "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU3RMMtieL6t"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_SVM_RESULTS): #decide if load results or not\n",
        "\n",
        "  Y_train_predicted = SVM_detector.predict(X_train_vector)\n",
        "\n",
        "  print(output.OKGREEN+\"SVM train results computed\"+output.END)\n",
        "\n",
        "else:\n",
        "  \n",
        "  !gdown --id 1ZKfjD5yxHkgjsD4K_HBT7gx50nDjssvh #SVM_results.npz\n",
        "  Y_train_predicted = np.load(\"SVM_results.npz\", mmap_mode = \"r\")[\"Y_train_predicted\"]\n",
        "  \n",
        "  print(output.OKGREEN+\"SVM train results loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuHwPobZXKuh"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "for prediction in Y_train_predicted:\n",
        "\n",
        "  if(prediction == 1):\n",
        "\n",
        "    correct = correct + 1\n",
        "\n",
        "print(\"TRAIN ACCURACY: \"+str(correct/IMAGES_TRAIN*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY-R0EabXpgv"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_SVM_RESULTS):\n",
        "\n",
        "  Y_test_predicted = SVM_detector.predict(X_test_vector)\n",
        "\n",
        "  print(output.OKGREEN+\"SVM test results computed\"+output.END)\n",
        "\n",
        "else:\n",
        "  \n",
        "  Y_test_predicted = np.load(\"SVM_results.npz\")[\"Y_test_predicted\"]\n",
        "\n",
        "  print(output.OKGREEN+\"SVM test results loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n1jJwLoXq8u"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "j = 0\n",
        "for prediction in Y_test_predicted:\n",
        "\n",
        "  if(prediction == 1 and Y_test[j]==0):\n",
        "\n",
        "    correct = correct + 1\n",
        "\n",
        "  if(prediction == -1 and Y_test[j]!=0):\n",
        "    \n",
        "    correct = correct + 1\n",
        "\n",
        "  j = j + 1\n",
        "\n",
        "print(\"TEST ACCURACY: \"+str(correct/IMAGES_TEST*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMVirkVratoU"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_SVM_RESULTS):\n",
        "  \n",
        "  np.savez(\"SVM_results.npz\", Y_train_predicted = Y_train_predicted, Y_test_predicted = Y_test_predicted )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjvxtSTVeFys"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_RESULTS_SVM):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp SVM_results.npz /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "\n",
        "    files.download(\"SVM_results.npz\")\n",
        "\n",
        "  \n",
        "\n",
        "  print(output.OKGREEN+\"SVM results downloaaded to local memory\"+output.END)\n",
        "\n",
        "if(DOWNLOAD_SVM_MODEL):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp SVM_model.pkl /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "    \n",
        "    files.download(\"SVM_model.pkl\")\n",
        "\n",
        "  print(output.OKGREEN+\"SVM model downloaded to local memory\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DecQfd6wbDXs"
      },
      "outputs": [],
      "source": [
        "#cleaning ram\n",
        "del Y_train_predicted\n",
        "del Y_test_predicted\n",
        "del SVM_detector\n",
        "gc.collect()\n",
        "\n",
        "#removed not used files\n",
        "!rm SVM_results.npz\n",
        "!rm SVM_model.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6eflUGj_Q0L"
      },
      "source": [
        "##Optimization for the isolation forest<br>\n",
        "\n",
        "From the images 9 features was extracted to reduce the complexity and to permit to analayze better the difference beetween anomalous and clean data:\n",
        "\n",
        "\n",
        "*   Sum of the values inside the pixels\n",
        "*   Count of the number of black/not black pixels\n",
        "*   Count of the rows/columns with only black pixels\n",
        "*   Min/Max index of a not \"only black\" row/column\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYyD0lTOO2UT"
      },
      "outputs": [],
      "source": [
        "#array of vectors --> array of matrixes\n",
        "if(not DATASETS_ALREADY_CREATED):\n",
        "  \n",
        "  X_train_matrix = X_train_vector.astype(\"float32\").reshape(IMAGES_TRAIN,256,256,1)\n",
        "  np.save(\"X_train_matrix\", X_train_matrix)\n",
        "  del X_train_matrix\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "  X_test_matrix = X_test_vector.astype(\"float32\").reshape(IMAGES_TEST,256,256,1)\n",
        "  np.save(\"X_test_matrix\", X_test_matrix)\n",
        "  del X_test_matrix\n",
        "  gc.collect()\n",
        "\n",
        "  print(output.OKGREEN+\"Dataset (matrixes) saved\"+output.END)\n",
        "\n",
        "  X_train_matrix = np.load(\"X_train_matrix.npy\", mmap_mode=\"r+\")\n",
        "  X_test_matrix = np.load(\"X_test_matrix.npy\", mmap_mode=\"r+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbxFax8TQkNg"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_DATASET): #downloading the datasets to local memory\n",
        "  \n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp X_train_matrix.npy /content/drive/MyDrive\n",
        "    !cp X_test_matrix.npy /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "\n",
        "    files.download(\"X_test_matrix.npy\")\n",
        "    files.download(\"X_test_matrix.npy\")\n",
        "\n",
        "  print(output.OKGREEN+\"Dataset downloaded to local memory\"+output.END)\n",
        "\n",
        "  !rm X_train_matrix.npy\n",
        "  !rm X_test_matrix.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK7jKbicOOTH"
      },
      "outputs": [],
      "source": [
        "############ TRAIN SET ############\n",
        "\n",
        "X_train_sum = [] #list of the vectors containing the features for each image\n",
        "\n",
        "for element in X_train_matrix:\n",
        "\n",
        "  temp = np.zeros(9) #vector used to save the features of the current image\n",
        "  temp [0]= element.sum() #sum of values\n",
        "  temp[1]=np.count_nonzero(element==0) #counting black pixels\n",
        "  temp[2]=np.count_nonzero(element!=0) #counting not black pixels\n",
        "\n",
        "  sum=np.sum(element, axis=0) #counting the values in the same row\n",
        "\n",
        "  zero_rows=0\n",
        "  indexes_row=[]\n",
        "\n",
        "  for i in range(len(sum)): #if sum = 0 => only black pixels in that row\n",
        "    if(sum[i]==0):\n",
        "      zero_rows=zero_rows+1\n",
        "    \n",
        "    else:\n",
        "      indexes_row.append(i) #index of a \"not only black pixels\" row\n",
        "\n",
        "  temp[3] = zero_rows #saving the number of \"only black pixels\" rows\n",
        "  temp[4] = max(indexes_row) #min index of a \"not only black pixels\" row\n",
        "  temp[5] = min(indexes_row) #max index of a \"not only black pixels\" row\n",
        "\n",
        "  sum=np.sum(element, axis=1) #counting the values in the same column\n",
        "\n",
        "  zero_col=0\n",
        "  indexes_col=[]\n",
        "\n",
        "  for i in range(len(sum)): #if sum = 0 => only black pixels in that column\n",
        "    if(sum[i]==0):\n",
        "      zero_col=zero_col+1\n",
        "    \n",
        "    else:\n",
        "      indexes_col.append(i) #index of a \"not only black pixels\" column\n",
        "\n",
        "  temp[6] = zero_col #saving the number of \"only black pixels\" columns\n",
        "  temp[7] = min(indexes_col) #min index of a \"not only black pixels\" colum\n",
        "  temp[8] = max(indexes_col) #max index of a \"not only black pixels\" colum\n",
        "\n",
        "  X_train_sum.append(temp) #adding the vector to the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikTdAFBXO7F-"
      },
      "outputs": [],
      "source": [
        "############ TEST SET ############\n",
        "\n",
        "X_test_sum = [] #list of the vectors containing the features for each image\n",
        "\n",
        "for element in X_test_matrix:\n",
        "\n",
        "  temp = np.zeros(9) #vector used to save the features of the current image\n",
        "  temp [0]= element.sum() #sum of values\n",
        "  temp[1]=np.count_nonzero(element==0) #counting black pixels\n",
        "  temp[2]=np.count_nonzero(element!=0) #counting not black pixels\n",
        "\n",
        "  sum=np.sum(element, axis=0) #counting the values in the same row\n",
        "\n",
        "  zero_rows=0 \n",
        "  indexes_row=[]\n",
        "\n",
        "  for i in range(len(sum)): #if sum = 0 => only black pixels in that row\n",
        "    if(sum[i]==0):\n",
        "      zero_rows=zero_rows+1\n",
        "    \n",
        "    else:\n",
        "      indexes_row.append(i) #index of a \"not only black pixels\" row\n",
        "\n",
        "  temp[3] = zero_rows  #saving the number of \"only black pixels\" rows\n",
        "  temp[4] = max(indexes_row) #min index of a \"not only black pixels\" row\n",
        "  temp[5] = min(indexes_row) #max index of a \"not only black pixels\" row\n",
        "\n",
        "\n",
        "  sum=np.sum(element, axis=1) #counting the values in the same column\n",
        "  \n",
        "  zero_col=0\n",
        "  indexes_col=[]\n",
        "\n",
        "  for i in range(len(sum)): #if sum = 0 => only black pixels in that column\n",
        "    \n",
        "    if(sum[i]==0):\n",
        "      zero_col=zero_col+1\n",
        "    \n",
        "    else:\n",
        "      indexes_col.append(i) #index of a \"not only black pixels\" column\n",
        "\n",
        "  temp[6] = zero_col #saving the number of \"only black pixels\" columns\n",
        "  temp[7] = min(indexes_col) #min index of a \"not only black pixels\" colum\n",
        "  temp[8] = max(indexes_col) #max index of a \"not only black pixels\" colum\n",
        "\n",
        "  X_test_sum.append(temp) #adding the vector to the train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ecEdg9gFNMZ"
      },
      "outputs": [],
      "source": [
        "  #training the model with the new features space\n",
        "  \n",
        "  IF_detector = IsolationForest(n_estimators = N_ESTIMATORS, max_samples=IMAGES_TRAIN,\n",
        "                                n_jobs = -1, random_state = 2053042)\n",
        "  \n",
        "  IF_detector.fit(X_train_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWEpU5t5FNA2"
      },
      "outputs": [],
      "source": [
        "Y_train_predicted=IF_detector.predict(X_train_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4mnjfwlFc_e"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for prediction in Y_train_predicted:\n",
        "\n",
        "  if(prediction == 1):\n",
        "\n",
        "    correct = correct + 1\n",
        "\n",
        "print(\"TRAIN ACCURACY: \"+str(correct/IMAGES_TRAIN*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVneZcBLFoHJ"
      },
      "outputs": [],
      "source": [
        "Y_test_predicted=IF_detector.predict(X_test_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OSr0M5IFvio"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "j = 0\n",
        "for prediction in Y_test_predicted:\n",
        "\n",
        "  if(prediction == 1 and Y_test[j]==0):\n",
        "\n",
        "    correct = correct +1\n",
        "\n",
        "  if(prediction == -1 and Y_test[j]!=0):\n",
        "    \n",
        "    correct = correct + 1\n",
        "\n",
        "  j = j + 1\n",
        "  \n",
        "print(\"TEST ACCURACY: \"+str(correct/IMAGES_TEST*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning useless lists\n",
        "\n",
        "del X_train_sum\n",
        "del X_test_sum\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XKTmNI2DcTGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFlj6jEtJG-K"
      },
      "source": [
        "##Autoencoder module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcwGNjsBFMPq"
      },
      "source": [
        "###Generation of the network\n",
        "\n",
        "Since i didn't work on NNs (except for the labs) i decided to follow the guides that can be found at these links:<br>\n",
        "[GUIDE 1](https://ai.plainenglish.io/convolutional-autoencoders-cae-with-tensorflow-97e8d8859cbe)<br>\n",
        "[GUIDE 2](https://www.analyticsvidhya.com/blog/2022/01/complete-guide-to-anomaly-detection-with-autoencoders-using-tensorflow/)<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwMOWi1-gm3c"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_AUTOENCODER):\n",
        "  \n",
        "  class AutoEncoder(Model):\n",
        "    def __init__(self):\n",
        "      super(AutoEncoder, self).__init__()\n",
        "      self.encoder = Sequential([\n",
        "                    Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\", input_shape=(256,256,1)),\n",
        "                    Flatten(),\n",
        "                    Dense(64, activation = \"relu\")\n",
        "                ])\n",
        "\n",
        "      self.decoder = Sequential([ \n",
        "                    Dense(64, activation=\"relu\", input_shape=self.encoder.output.shape[1:]),                                   \n",
        "                    Dense(128*128*32, activation=\"relu\"),\n",
        "                    Reshape((128,128,32)),\n",
        "                    Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\"),\n",
        "                    Conv2D(1, 3, strides=1, padding=\"same\", activation=\"relu\")\n",
        "                ])\n",
        "      \n",
        "    def call(self, x):\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)\n",
        "      return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6KsoRlXvP0L"
      },
      "outputs": [],
      "source": [
        "#Definition of a custom loss funtion\n",
        "def mse_custom(y_true, y_pred):\n",
        "\n",
        "  \n",
        "  true_1 = tf.boolean_mask(tf.convert_to_tensor(y_true), tf.math.greater(y_true,0))\n",
        "  true_0 = tf.boolean_mask(tf.convert_to_tensor(y_true), tf.math.less_equal(y_true,0))\n",
        "  pred_1 = tf.boolean_mask(y_pred, tf.math.greater(y_true,0))\n",
        "  pred_0 = tf.boolean_mask(y_pred, tf.math.less_equal(y_true,0))\n",
        "\n",
        "\n",
        "  loss = (tf.math.reduce_sum(tf.math.square(tf.math.subtract(true_1, pred_1))) +\n",
        "          0.0001*tf.math.reduce_sum(tf.math.square(tf.math.subtract(true_0, pred_0)))/65536)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIg0WbhknIaL"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_AUTOENCODER):\n",
        "\n",
        "  #count the number of data for the train and for the validation\n",
        "  data = X_train_matrix.shape[0]\n",
        "  validation = int(data*0.2)\n",
        "\n",
        "\n",
        "  #definition of the callback function\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, mode=\"min\")\n",
        "\n",
        "  model = AutoEncoder()\n",
        "  model.compile(optimizer=\"adam\", loss = mse_custom, steps_per_execution = 1)\n",
        "\n",
        "  model.encoder.summary()\n",
        "  model.decoder.summary()\n",
        "\n",
        "  history = model.fit(X_train_matrix[0:(data-validation)], \n",
        "                      X_train_matrix[0:(data-validation)], \n",
        "                      epochs = 100, batch_size = BATCH_SIZE,\n",
        "                      validation_data =\n",
        "                      (X_train_matrix[(data-validation):data], \n",
        "                      X_train_matrix[(data-validation):data]),\n",
        "                      shuffle=False,\n",
        "                      callbacks=[early_stopping])\n",
        "  \n",
        "  model.save(\"AUTOENCODER_model\")\n",
        "  np.save(\"AUTOENCODER_history.npy\", history.history)\n",
        "  print(output.OKGREEN+\"Autoencoder saved\"+output.END)\n",
        "\n",
        "else:\n",
        "\n",
        "  !gdown --id 1CMmv9NIaiP5dxCjjlr4CtSBOKsT7o8a4 #AUTOENCODER_model.zip\n",
        "  !gdown --id 1NdjEPNez1-GBvWMW7eL0h6zoMCORtfxU #AUTOENCODER_history.npy\n",
        "\n",
        "  !unzip AUTOENCODER_model.zip\n",
        "\n",
        "  model = keras.models.load_model(\"content/AUTOENCODER_model\", compile = False)\n",
        "\n",
        "  model.encoder.summary()\n",
        "  model.decoder.summary()\n",
        "\n",
        "  history = np.load(\"AUTOENCODER_history.npy\", allow_pickle = \"TRUE\").item()\n",
        "\n",
        "  print(output.OKGREEN+\"Autoencoder loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OipVlSLhnvGx"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_AUTOENCODER_MODEL):\n",
        "\n",
        "  !zip -r /content/AUTOENCODER_model.zip /content/AUTOENCODER_model\n",
        "  \n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp AUTOENCODER_model.zip /content/drive/MyDrive\n",
        "    !cp AUTOENCODER_history.npy /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "\n",
        "    files.download(\"AUTOENCODER_model.zip\")\n",
        "    files.download(\"AUTOENCODER_history.npy\")\n",
        "\n",
        "  print(output.OKGREEN+\"Autoencoder downloaded to local memory\"+output.END)\n",
        "  !rm AUTOENCODER_model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4_MBVgAKKlL"
      },
      "source": [
        "###Evaluation of the perfomace: training error and test error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdrzD159r2er"
      },
      "source": [
        "####Train error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pANDw2YDDyZs"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NXX7JCGJhMx"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_AUTOENCODER_RESULTS):\n",
        "   \n",
        "  tmp = model.predict(X_train_matrix, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR3kqh_xeKDf"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_AUTOENCODER_RESULTS): #decide if load the autoencoder results\n",
        "  \n",
        "  np.save(\"AUTOENCODER_results_train\", tmp)\n",
        "  del tmp\n",
        "  gc.collect()\n",
        "  train_reconstruction=np.load(\"AUTOENCODER_results_train.npy\", mmap_mode=\"r+\")\n",
        "\n",
        "  print(output.OKGREEN+\"Autoencoder train results saved\"+output.END)\n",
        "\n",
        "else:\n",
        "\n",
        "  !gdown --id 1maNMJZnnAAK0D0pt1LnYZ4gyVmrcXZQk #AUTOENCODER_results_train.npy\n",
        "\n",
        "  train_reconstruction = np.load(\"AUTOENCODER_results_train.npy\", mmap_mode = \"r\")\n",
        "\n",
        "  print(output.OKGREEN+\"Autoencoder train results loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ab28s3KLOxw"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_RESULTS_AUTOENCODER):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp AUTOENCODER_results_train.npy /content/drive/MyDrive\n",
        "  \n",
        "  else:\n",
        "    files.download(\"AUTOENCODER_results_train.npy\")\n",
        "  \n",
        "  print(output.OKGREEN+\"Autoencoder train results downloaded\"+output.END)\n",
        "\n",
        "!rm AUTOENCODER_results_train.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0FVRvaAqE9n"
      },
      "source": [
        "#####Compute the losses and set the threshold "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zz-YO8JWdfZ"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "\n",
        "for i in range(train_reconstruction.shape[0]):\n",
        "\n",
        "  train_losses.append(mse_custom(X_train_matrix[i],train_reconstruction[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te69seCvq79I"
      },
      "outputs": [],
      "source": [
        "threshold = np.mean(train_losses) + 0.8*np.std(train_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Q34fXMrJyo"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for loss in train_losses:\n",
        "  if(loss <= threshold):\n",
        "    correct = correct + 1\n",
        "\n",
        "print(\"TRAIN ACCURACY: \"+str(correct/IMAGES_TRAIN*100)+\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIqjXNLSr82l"
      },
      "source": [
        "####Test error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9MY0Z-9dmJ9"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BBy8Q-ir_7w"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_AUTOENCODER_RESULTS):\n",
        "    \n",
        "  tmp = model.predict(X_test_matrix, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noZGhRjLsGX2"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_AUTOENCODER_RESULTS): #decide if load the autoencoder results\n",
        "  \n",
        "  np.save(\"AUTOENCODER_results_test\", tmp)\n",
        "  del tmp\n",
        "  gc.collect()\n",
        "  test_reconstruction = np.load(\"AUTOENCODER_results_test.npy\", mmap_mode=\"r+\")\n",
        "  print(output.OKGREEN+\"Autoencoder test results saved\"+output.END)\n",
        "\n",
        "else:\n",
        "\n",
        "  !gdown --id 1e8e0qyFWlWSr6WIoJjBmOXo4w38d4NA9 #AUTOENCODER_results_test.npy\n",
        "  test_reconstruction = np.load(\"AUTOENCODER_results_test.npy\", mmap_mode = \"r\")\n",
        "\n",
        "  print(output.OKGREEN+\"Autoencoder test results loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neU5AK2StbkH"
      },
      "outputs": [],
      "source": [
        "if(DOWNLOAD_RESULTS_AUTOENCODER):\n",
        "\n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp AUTOENCODER_results_test.npy /content/drive/MyDrive\n",
        "  \n",
        "  else:\n",
        "    \n",
        "    files.download(\"AUTOENCODER_results_test.npy\")\n",
        "  \n",
        "  print(output.OKGREEN+\"Autoencoder test results downloaded\"+output.END)\n",
        "\n",
        "!rm AUTOENCODER_results_test.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJyuecwrXYq-"
      },
      "outputs": [],
      "source": [
        "test_losses = []\n",
        "\n",
        "for i in range(test_reconstruction.shape[0]):\n",
        "\n",
        "  test_losses.append(mse_custom(X_test_matrix[i],test_reconstruction[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMklYvk-sWqS"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "i = 0\n",
        "\n",
        "for loss in test_losses:\n",
        "\n",
        "  if(loss <= threshold and Y_test[i] == 0):\n",
        "\n",
        "    correct = correct + 1\n",
        "\n",
        "  if(loss > threshold and Y_test[i] != 0):\n",
        "\n",
        "    correct = correct + 1\n",
        "\n",
        "  i = i + 1\n",
        "\n",
        "print(\"TEST ACCURACY: \"+str(correct/IMAGES_TEST*100)+\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MFBM6YPkmnf"
      },
      "source": [
        "##Classification module\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [\"clean\",\"dos11\",\"dos53\",\"scan11\",\"scan44\"]\n",
        "y = np.unique(Y_train_classification, return_counts=True)[1]\n",
        "\n",
        "fig, ax = plt.subplots()    \n",
        "width = 0.75 # the width of the bars \n",
        "ind = np.arange(len(y))  # the x locations for the groups\n",
        "ax.barh(ind, y, width, color=[\"blue\",\"red\",\"red\",\"red\", \"red\"])\n",
        "ax.set_yticks(ind+width/2)\n",
        "ax.set_yticklabels(x, minor=False)\n",
        "plt.title(\"Train set for classification data distribution\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")      \n",
        "for i, v in enumerate(y):\n",
        "    ax.text(v + 3, i + .25, str(v), color=\"black\", fontweight=\"bold\")\n",
        "plt.savefig(os.path.join(\"Train_classification.png\"), dpi=300, format=\"png\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "NSjDeL9DkfrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApgnDxFJkl5S"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, labels, predictions=None, class_names=None):\n",
        "    assert len(images) == len(labels) == 9\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        \n",
        "        # Plot image.\n",
        "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "        \n",
        "        # Show true and predicted classes.\n",
        "        if predictions is None:\n",
        "            xlabel = \"True: {0}\".format(class_names[int(labels[i])])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(class_names[int(labels[i,0])], class_names[int(predictions[i].argmax())])\n",
        "        ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKTdjZ7yk_No"
      },
      "source": [
        "**LABELS:**<br>\n",
        "clean = 0, <br>\n",
        "dos11 = 1,<br>\n",
        "dos53 = 2,<br>\n",
        "scan11 = 3,<br>\n",
        "scan44 = 4.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a69BUfnZk05B"
      },
      "outputs": [],
      "source": [
        "#define the classes\n",
        "label_ids = {'Clean': 0, 'dos11': 1, 'dos53': 2, 'scan11': 3, 'scan44': 4}\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXiybEqdlLYx"
      },
      "outputs": [],
      "source": [
        "class_names = {value:key for key, value in label_ids.items()}\n",
        "plot_images(\n",
        "    X_train_classification[[1,700,1400,2100,2800,3500,4200,4900,5600]],\n",
        "    Y_train_classification[[1,700,1400,2100,2800,3500,4200,4900,5600]],\n",
        "    predictions=None,\n",
        "    class_names=class_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdPRtB1umCgd"
      },
      "outputs": [],
      "source": [
        "#shuffle sets using the shuffle function from sklearn (provided above)\n",
        "train_images, train_labels = shuffle(X_train_classification, Y_train_classification, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1_WOz3MmMkL"
      },
      "outputs": [],
      "source": [
        "# Number of samples and size of one of the image dimension (images are squared)\n",
        "num_samples = len(X_train_classification)\n",
        "img_shape = X_train_classification.shape\n",
        "\n",
        "print(img_shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\", input_shape=(256,256,1)),\n",
        "    Conv2D(16, 3, strides=2, padding=\"same\", activation=\"relu\"),\n",
        "    Flatten(),\n",
        "    Dense(128, activation = \"relu\"),\n",
        "    Dense(64, activation = \"relu\"),\n",
        "    Dense(32, activation = \"relu\"),\n",
        "    Dense(5, activation = \"softmax\"),\n",
        "])"
      ],
      "metadata": {
        "id": "g0g_cfDzx8c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vey-Md5TmZ6l"
      },
      "outputs": [],
      "source": [
        "if(not LOAD_CLASSIFICATOR):\n",
        "  \n",
        "  #definition of the callback function\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
        "                                                    patience=4, mode=\"max\")\n",
        "  model.compile(optimizer=\"adam\", \n",
        "                loss= tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "  history = model.fit(train_images, train_labels, \n",
        "                      batch_size = BATCH_SIZE , epochs = N_EPOCHS_C, \n",
        "                      validation_split = 0.2,\n",
        "                      callbacks = [early_stopping])\n",
        "  \n",
        "  model.save(\"CLASSIFICATOR_model\")\n",
        "  np.save(\"CLASSIFICATOR_history.npy\", history.history)\n",
        "  print(output.OKGREEN+\"Classificator saved\"+output.END)\n",
        "\n",
        "else:\n",
        "\n",
        "  !gdown --id 15EdZ-DjlQJWQp0TcIAXt5EaPG3dRAeKF #CLASSIFICATOR_model.zip\n",
        "  !gdown --id 1gRQL4pM5yHiXiVCURGbMKpAhhTRjyUeW #CLASSIFICATOR_history.npy\n",
        "\n",
        "  !unzip CLASSIFICATOR_model.zip\n",
        "\n",
        "  model = keras.models.load_model(\"content/CLASSIFICATOR_model\")\n",
        "\n",
        "\n",
        "\n",
        "  history = np.load(\"CLASSIFICATOR_history.npy\", allow_pickle = \"TRUE\").item()\n",
        "\n",
        "  print(output.OKGREEN+\"Classificator loaded\"+output.END)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(DOWNLOAD_CLASSIFICATOR_MODEL):\n",
        "\n",
        "  !zip -r /content/CLASSIFICATOR_model.zip /content/CLASSIFICATOR_model\n",
        "  \n",
        "  if(GOOGLE_DRIVE):\n",
        "\n",
        "    !cp CLASSIFICATOR_model.zip /content/drive/MyDrive\n",
        "    !cp CLASSIFICATOR_history.npy /content/drive/MyDrive\n",
        "\n",
        "  else:\n",
        "\n",
        "    files.download(\"CLASSIFICATOR_model.zip\")\n",
        "    files.download(\"CLASSIFICATOR_history.npy\")\n",
        "\n",
        "  print(output.OKGREEN+\"Classificator downloaded to local memory\"+output.END)\n",
        "  !rm CLASSIFICATOR_model.zip"
      ],
      "metadata": {
        "id": "o_-E20Axpczq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF8ASojjmy-F"
      },
      "outputs": [],
      "source": [
        "# shuffle sets\n",
        "test_images, test_labels = shuffle(X_test_matrix, Y_test, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHHraliEm-a7"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(\"TEST ACCURACY: \" +str(test_acc*100)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akuk7si9qyOV"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, labels, predictions=None, class_names=None):\n",
        "    assert len(images) == len(labels) == 9\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        \n",
        "        # Plot image.\n",
        "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "        \n",
        "        # Show true and predicted classes.\n",
        "        if predictions is None:\n",
        "            xlabel = \"True: {0}\".format(class_names[int(labels[i])])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(class_names[int(labels[i])], class_names[int(predictions[i].argmax())])\n",
        "        ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFiaGqKlnBA4"
      },
      "outputs": [],
      "source": [
        "plot_images(images=test_images[[1, 300, 600, 900, 1200, 1500, 1800,2100,2400]], labels=test_labels[[1, 300, 600, 900, 1200, 1500, 1800,2100,2400]],\n",
        "            predictions=model.predict(test_images[[1, 300, 600, 900, 1200, 1500, 1800,2100,2400]]), class_names = class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReKHU_dZm8TY"
      },
      "outputs": [],
      "source": [
        "def print_confusion_matrix_pandas(model, images, labels):\n",
        "    num_classes = 5\n",
        "    # Get the predicted classifications for the test-set.\n",
        "    predictions = model.predict(images)\n",
        "    # Get the confusion matrix using sklearn.\n",
        "    cm = confusion_matrix(y_true=labels, y_pred=np.argmax(predictions,axis=1))\n",
        "    # Plot the confusion matrix as an image.\n",
        "    class_names = label_ids.keys()\n",
        "    df_cm = pd.DataFrame(cm, index = class_names, columns = class_names)\n",
        "    plt.figure(figsize = (15,10))\n",
        "    sn.heatmap(df_cm, annot=True, cmap='Blues')\n",
        "    #plt.axis([-0.5, 13.5, 13.5, -0.5])\n",
        "    plt.title('Confusion Matrix Test Set')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tppLSioanpmJ"
      },
      "outputs": [],
      "source": [
        "print_confusion_matrix_pandas(model, test_images, test_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-EOPx1UNeBmC",
        "2bvSZ0JbQMHw",
        "Y_K5N-w4rodm",
        "Mj0HgPADPALm",
        "UlSfu3Og8f14",
        "uRIywU3oHnSn"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}